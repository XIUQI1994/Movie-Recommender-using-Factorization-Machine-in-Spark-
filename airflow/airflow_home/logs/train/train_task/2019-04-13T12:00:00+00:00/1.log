[2019-04-14 17:57:25,155] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: train.train_task 2019-04-13T12:00:00+00:00 [queued]>
[2019-04-14 17:57:25,166] {__init__.py:1139} INFO - Dependencies all met for <TaskInstance: train.train_task 2019-04-13T12:00:00+00:00 [queued]>
[2019-04-14 17:57:25,166] {__init__.py:1353} INFO - 
--------------------------------------------------------------------------------
[2019-04-14 17:57:25,166] {__init__.py:1354} INFO - Starting attempt 1 of 1
[2019-04-14 17:57:25,166] {__init__.py:1355} INFO - 
--------------------------------------------------------------------------------
[2019-04-14 17:57:25,180] {__init__.py:1374} INFO - Executing <Task(PythonOperator): train_task> on 2019-04-13T12:00:00+00:00
[2019-04-14 17:57:25,180] {base_task_runner.py:119} INFO - Running: ['airflow', 'run', 'train', 'train_task', '2019-04-13T12:00:00+00:00', '--job_id', '6', '--raw', '-sd', 'DAGS_FOLDER/train.py', '--cfg_path', '/tmp/tmp85xyr3v1']
[2019-04-14 17:57:25,569] {base_task_runner.py:101} INFO - Job 6: Subtask train_task /home/xiuqi/.local/lib/python3.7/site-packages/airflow/configuration.py:590: DeprecationWarning: You have two airflow.cfg files: /home/xiuqi/airflow/airflow.cfg and /home/xiuqi/Dropbox/MLE/myProject/airflow/airflow_home/airflow.cfg. Airflow used to look at ~/airflow/airflow.cfg, even when AIRFLOW_HOME was set to a different value. Airflow will now only read /home/xiuqi/Dropbox/MLE/myProject/airflow/airflow_home/airflow.cfg, and you should remove the other file
[2019-04-14 17:57:25,569] {base_task_runner.py:101} INFO - Job 6: Subtask train_task   category=DeprecationWarning,
[2019-04-14 17:57:25,734] {base_task_runner.py:101} INFO - Job 6: Subtask train_task [2019-04-14 17:57:25,734] {__init__.py:51} INFO - Using executor SequentialExecutor
[2019-04-14 17:57:25,919] {base_task_runner.py:101} INFO - Job 6: Subtask train_task [2019-04-14 17:57:25,919] {__init__.py:305} INFO - Filling up the DagBag from /home/xiuqi/Dropbox/MLE/myProject/airflow/airflow_home/dags/train.py
[2019-04-14 17:57:26,043] {base_task_runner.py:101} INFO - Job 6: Subtask train_task [2019-04-14 17:57:26,042] {cli.py:517} INFO - Running <TaskInstance: train.train_task 2019-04-13T12:00:00+00:00 [running]> on host xiuqi-debian
[2019-04-14 17:57:26,060] {python_operator.py:104} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_ID=train
AIRFLOW_CTX_TASK_ID=train_task
AIRFLOW_CTX_EXECUTION_DATE=2019-04-13T12:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2019-04-13T12:00:00+00:00
[2019-04-14 17:57:27,288] {base_task_runner.py:101} INFO - Job 6: Subtask train_task Ivy Default Cache set to: /home/xiuqi/.ivy2/cache
[2019-04-14 17:57:27,288] {base_task_runner.py:101} INFO - Job 6: Subtask train_task The jars for the packages stored in: /home/xiuqi/.ivy2/jars
[2019-04-14 17:57:27,329] {base_task_runner.py:101} INFO - Job 6: Subtask train_task :: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2019-04-14 17:57:27,455] {base_task_runner.py:101} INFO - Job 6: Subtask train_task org.mongodb.spark#mongo-spark-connector_2.11 added as a dependency
[2019-04-14 17:57:27,457] {base_task_runner.py:101} INFO - Job 6: Subtask train_task :: resolving dependencies :: org.apache.spark#spark-submit-parent-45f19c2f-8673-420b-9e32-3f327c03bf71;1.0
[2019-04-14 17:57:27,457] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	confs: [default]
[2019-04-14 17:57:27,631] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	found org.mongodb.spark#mongo-spark-connector_2.11;2.4.0 in spark-list
[2019-04-14 17:57:27,665] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	found org.mongodb#mongo-java-driver;3.9.0 in spark-list
[2019-04-14 17:57:27,691] {base_task_runner.py:101} INFO - Job 6: Subtask train_task :: resolution report :: resolve 227ms :: artifacts dl 7ms
[2019-04-14 17:57:27,691] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	:: modules in use:
[2019-04-14 17:57:27,693] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	org.mongodb#mongo-java-driver;3.9.0 from spark-list in [default]
[2019-04-14 17:57:27,694] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	org.mongodb.spark#mongo-spark-connector_2.11;2.4.0 from spark-list in [default]
[2019-04-14 17:57:27,694] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	---------------------------------------------------------------------
[2019-04-14 17:57:27,694] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	|                  |            modules            ||   artifacts   |
[2019-04-14 17:57:27,694] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2019-04-14 17:57:27,694] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	---------------------------------------------------------------------
[2019-04-14 17:57:27,694] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |
[2019-04-14 17:57:27,694] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	---------------------------------------------------------------------
[2019-04-14 17:57:27,701] {base_task_runner.py:101} INFO - Job 6: Subtask train_task :: retrieving :: org.apache.spark#spark-submit-parent-45f19c2f-8673-420b-9e32-3f327c03bf71
[2019-04-14 17:57:27,701] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	confs: [default]
[2019-04-14 17:57:27,709] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	0 artifacts copied, 2 already retrieved (0kB/8ms)
[2019-04-14 17:57:27,804] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 2019-04-14 17:57:27 WARN  Utils:66 - Your hostname, xiuqi-debian resolves to a loopback address: 127.0.1.1; using 192.168.1.191 instead (on interface wlp2s0)
[2019-04-14 17:57:27,805] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 2019-04-14 17:57:27 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address
[2019-04-14 17:57:28,166] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 2019-04-14 17:57:28 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2019-04-14 17:57:28,666] {base_task_runner.py:101} INFO - Job 6: Subtask train_task Setting default log level to "WARN".
[2019-04-14 17:57:28,666] {base_task_runner.py:101} INFO - Job 6: Subtask train_task To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[2019-04-14 17:57:29,820] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 2019-04-14 17:57:29 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2019-04-14 17:57:31,505] {__init__.py:1580} ERROR - An error occurred while calling o49.addFile.
: java.io.FileNotFoundException: File file:/home/xiuqi/Dropbox/MLE/myProject/airflow/fm/fm_parallel_extend.py does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.spark.SparkContext.addFile(SparkContext.scala:1544)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Traceback (most recent call last):
  File "/home/xiuqi/.local/lib/python3.7/site-packages/airflow/models/__init__.py", line 1441, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/home/xiuqi/.local/lib/python3.7/site-packages/airflow/operators/python_operator.py", line 112, in execute
    return_value = self.execute_callable()
  File "/home/xiuqi/.local/lib/python3.7/site-packages/airflow/operators/python_operator.py", line 117, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/home/xiuqi/Dropbox/MLE/myProject/airflow/airflow_home/dags/train.py", line 11, in train_task
    a.do()
  File "/home/xiuqi/Dropbox/MLE/myProject/airflow/airflow_home/dags/interval_train.py", line 32, in do
    sc.addPyFile("./fm/fm_parallel_extend.py")
  File "/home/xiuqi/.local/lib/python3.7/site-packages/pyspark/context.py", line 909, in addPyFile
    self.addFile(path)
  File "/home/xiuqi/.local/lib/python3.7/site-packages/pyspark/context.py", line 898, in addFile
    self._jsc.sc().addFile(path, recursive)
  File "/home/xiuqi/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1257, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/home/xiuqi/.local/lib/python3.7/site-packages/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/home/xiuqi/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o49.addFile.
: java.io.FileNotFoundException: File file:/home/xiuqi/Dropbox/MLE/myProject/airflow/fm/fm_parallel_extend.py does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
	at org.apache.spark.SparkContext.addFile(SparkContext.scala:1544)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

[2019-04-14 17:57:31,509] {__init__.py:1611} INFO - Marking task as FAILED.
[2019-04-14 17:57:31,588] {base_task_runner.py:101} INFO - Job 6: Subtask train_task Traceback (most recent call last):
[2019-04-14 17:57:31,589] {base_task_runner.py:101} INFO - Job 6: Subtask train_task   File "/home/xiuqi/.local/bin/airflow", line 32, in <module>
[2019-04-14 17:57:31,589] {base_task_runner.py:101} INFO - Job 6: Subtask train_task     args.func(args)
[2019-04-14 17:57:31,589] {base_task_runner.py:101} INFO - Job 6: Subtask train_task   File "/home/xiuqi/.local/lib/python3.7/site-packages/airflow/utils/cli.py", line 74, in wrapper
[2019-04-14 17:57:31,589] {base_task_runner.py:101} INFO - Job 6: Subtask train_task     return f(*args, **kwargs)
[2019-04-14 17:57:31,589] {base_task_runner.py:101} INFO - Job 6: Subtask train_task   File "/home/xiuqi/.local/lib/python3.7/site-packages/airflow/bin/cli.py", line 523, in run
[2019-04-14 17:57:31,589] {base_task_runner.py:101} INFO - Job 6: Subtask train_task     _run(args, dag, ti)
[2019-04-14 17:57:31,589] {base_task_runner.py:101} INFO - Job 6: Subtask train_task   File "/home/xiuqi/.local/lib/python3.7/site-packages/airflow/bin/cli.py", line 442, in _run
[2019-04-14 17:57:31,589] {base_task_runner.py:101} INFO - Job 6: Subtask train_task     pool=args.pool,
[2019-04-14 17:57:31,589] {base_task_runner.py:101} INFO - Job 6: Subtask train_task   File "/home/xiuqi/.local/lib/python3.7/site-packages/airflow/utils/db.py", line 73, in wrapper
[2019-04-14 17:57:31,589] {base_task_runner.py:101} INFO - Job 6: Subtask train_task     return func(*args, **kwargs)
[2019-04-14 17:57:31,589] {base_task_runner.py:101} INFO - Job 6: Subtask train_task   File "/home/xiuqi/.local/lib/python3.7/site-packages/airflow/models/__init__.py", line 1441, in _run_raw_task
[2019-04-14 17:57:31,589] {base_task_runner.py:101} INFO - Job 6: Subtask train_task     result = task_copy.execute(context=context)
[2019-04-14 17:57:31,589] {base_task_runner.py:101} INFO - Job 6: Subtask train_task   File "/home/xiuqi/.local/lib/python3.7/site-packages/airflow/operators/python_operator.py", line 112, in execute
[2019-04-14 17:57:31,589] {base_task_runner.py:101} INFO - Job 6: Subtask train_task     return_value = self.execute_callable()
[2019-04-14 17:57:31,589] {base_task_runner.py:101} INFO - Job 6: Subtask train_task   File "/home/xiuqi/.local/lib/python3.7/site-packages/airflow/operators/python_operator.py", line 117, in execute_callable
[2019-04-14 17:57:31,589] {base_task_runner.py:101} INFO - Job 6: Subtask train_task     return self.python_callable(*self.op_args, **self.op_kwargs)
[2019-04-14 17:57:31,589] {base_task_runner.py:101} INFO - Job 6: Subtask train_task   File "/home/xiuqi/Dropbox/MLE/myProject/airflow/airflow_home/dags/train.py", line 11, in train_task
[2019-04-14 17:57:31,590] {base_task_runner.py:101} INFO - Job 6: Subtask train_task     a.do()
[2019-04-14 17:57:31,590] {base_task_runner.py:101} INFO - Job 6: Subtask train_task   File "/home/xiuqi/Dropbox/MLE/myProject/airflow/airflow_home/dags/interval_train.py", line 32, in do
[2019-04-14 17:57:31,590] {base_task_runner.py:101} INFO - Job 6: Subtask train_task     sc.addPyFile("./fm/fm_parallel_extend.py")
[2019-04-14 17:57:31,590] {base_task_runner.py:101} INFO - Job 6: Subtask train_task   File "/home/xiuqi/.local/lib/python3.7/site-packages/pyspark/context.py", line 909, in addPyFile
[2019-04-14 17:57:31,590] {base_task_runner.py:101} INFO - Job 6: Subtask train_task     self.addFile(path)
[2019-04-14 17:57:31,590] {base_task_runner.py:101} INFO - Job 6: Subtask train_task   File "/home/xiuqi/.local/lib/python3.7/site-packages/pyspark/context.py", line 898, in addFile
[2019-04-14 17:57:31,590] {base_task_runner.py:101} INFO - Job 6: Subtask train_task     self._jsc.sc().addFile(path, recursive)
[2019-04-14 17:57:31,590] {base_task_runner.py:101} INFO - Job 6: Subtask train_task   File "/home/xiuqi/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1257, in __call__
[2019-04-14 17:57:31,590] {base_task_runner.py:101} INFO - Job 6: Subtask train_task     answer, self.gateway_client, self.target_id, self.name)
[2019-04-14 17:57:31,590] {base_task_runner.py:101} INFO - Job 6: Subtask train_task   File "/home/xiuqi/.local/lib/python3.7/site-packages/pyspark/sql/utils.py", line 63, in deco
[2019-04-14 17:57:31,590] {base_task_runner.py:101} INFO - Job 6: Subtask train_task     return f(*a, **kw)
[2019-04-14 17:57:31,590] {base_task_runner.py:101} INFO - Job 6: Subtask train_task   File "/home/xiuqi/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
[2019-04-14 17:57:31,590] {base_task_runner.py:101} INFO - Job 6: Subtask train_task     format(target_id, ".", name), value)
[2019-04-14 17:57:31,590] {base_task_runner.py:101} INFO - Job 6: Subtask train_task py4j.protocol.Py4JJavaError: An error occurred while calling o49.addFile.
[2019-04-14 17:57:31,590] {base_task_runner.py:101} INFO - Job 6: Subtask train_task : java.io.FileNotFoundException: File file:/home/xiuqi/Dropbox/MLE/myProject/airflow/fm/fm_parallel_extend.py does not exist
[2019-04-14 17:57:31,590] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)
[2019-04-14 17:57:31,590] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)
[2019-04-14 17:57:31,590] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)
[2019-04-14 17:57:31,591] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)
[2019-04-14 17:57:31,591] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	at org.apache.spark.SparkContext.addFile(SparkContext.scala:1544)
[2019-04-14 17:57:31,591] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2019-04-14 17:57:31,591] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2019-04-14 17:57:31,591] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2019-04-14 17:57:31,591] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	at java.lang.reflect.Method.invoke(Method.java:498)
[2019-04-14 17:57:31,591] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2019-04-14 17:57:31,591] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2019-04-14 17:57:31,591] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	at py4j.Gateway.invoke(Gateway.java:282)
[2019-04-14 17:57:31,591] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2019-04-14 17:57:31,591] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2019-04-14 17:57:31,591] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	at py4j.GatewayConnection.run(GatewayConnection.java:238)
[2019-04-14 17:57:31,591] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 	at java.lang.Thread.run(Thread.java:748)
[2019-04-14 17:57:31,591] {base_task_runner.py:101} INFO - Job 6: Subtask train_task 
[2019-04-14 17:57:35,158] {logging_mixin.py:95} INFO - [2019-04-14 17:57:35,157] {jobs.py:2562} INFO - Task exited with return code 1
